{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lending Club Loan Data \n",
    "### Analyzing Lending Club's issued loans\n",
    "### Code by: Sejal Jaiswal\n",
    "--> https://www.kaggle.com/wendykan/lending-club-loan-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import ensemble\n",
    "from sklearn.metrics import mean_squared_error\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "import plotly\n",
    "from wordcloud import WordCloud\n",
    "#from wordcloud import STOPWORDS\n",
    "import tensorflow as tf\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step1: Data understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loan_data = pd.read_csv(\"/Users/Sejal/Desktop/LoanProject/loan.csv\", sep = \",\")\n",
    "print(\"Data shape is: {}\".format(loan_data.shape))\n",
    "print(\"Column names: {}\".format(loan_data.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Visualization - Address"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working with the address data: 'addr_state'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "short_state_names = {\n",
    "        'AK': 'Alaska',\n",
    "        'AL': 'Alabama',\n",
    "        'AR': 'Arkansas',\n",
    "        'AS': 'American Samoa',\n",
    "        'AZ': 'Arizona',\n",
    "        'CA': 'California',\n",
    "        'CO': 'Colorado',\n",
    "        'CT': 'Connecticut',\n",
    "        'DC': 'District of Columbia',\n",
    "        'DE': 'Delaware',\n",
    "        'FL': 'Florida',\n",
    "        'GA': 'Georgia',\n",
    "        'GU': 'Guam',\n",
    "        'HI': 'Hawaii',\n",
    "        'IA': 'Iowa',\n",
    "        'ID': 'Idaho',\n",
    "        'IL': 'Illinois',\n",
    "        'IN': 'Indiana',\n",
    "        'KS': 'Kansas',\n",
    "        'KY': 'Kentucky',\n",
    "        'LA': 'Louisiana',\n",
    "        'MA': 'Massachusetts',\n",
    "        'MD': 'Maryland',\n",
    "        'ME': 'Maine',\n",
    "        'MI': 'Michigan',\n",
    "        'MN': 'Minnesota',\n",
    "        'MO': 'Missouri',\n",
    "        'MP': 'Northern Mariana Islands',\n",
    "        'MS': 'Mississippi',\n",
    "        'MT': 'Montana',\n",
    "        'NA': 'National',\n",
    "        'NC': 'North Carolina',\n",
    "        'ND': 'North Dakota',\n",
    "        'NE': 'Nebraska',\n",
    "        'NH': 'New Hampshire',\n",
    "        'NJ': 'New Jersey',\n",
    "        'NM': 'New Mexico',\n",
    "        'NV': 'Nevada',\n",
    "        'NY': 'New York',\n",
    "        'OH': 'Ohio',\n",
    "        'OK': 'Oklahoma',\n",
    "        'OR': 'Oregon',\n",
    "        'PA': 'Pennsylvania',\n",
    "        'PR': 'Puerto Rico',\n",
    "        'RI': 'Rhode Island',\n",
    "        'SC': 'South Carolina',\n",
    "        'SD': 'South Dakota',\n",
    "        'TN': 'Tennessee',\n",
    "        'TX': 'Texas',\n",
    "        'UT': 'Utah',\n",
    "        'VA': 'Virginia',\n",
    "        'VI': 'Virgin Islands',\n",
    "        'VT': 'Vermont',\n",
    "        'WA': 'Washington',\n",
    "        'WI': 'Wisconsin',\n",
    "        'WV': 'West Virginia',\n",
    "        'WY': 'Wyoming'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "state_grp = loan_data.groupby('addr_state')\n",
    "mean_values = state_grp.aggregate(np.mean)\n",
    "max_values = state_grp.aggregate(np.max)\n",
    "min_values = state_grp.aggregate(np.min)\n",
    "#print(max_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mean_value_loan_amnt = []\n",
    "max_int_rate = []\n",
    "min_int_rate = []\n",
    "\n",
    "#Checking the mean loan amount in each state\n",
    "for i in state_name: \n",
    "    for key in mean_values['loan_amnt'].keys():\n",
    "        if i == key:\n",
    "            mean_value_loan_amnt.append(mean_values.loan_amnt[key])\n",
    "\n",
    "#Checking the maximum interest rate in each state\n",
    "for i in state_name: \n",
    "    for key in max_values['int_rate'].keys():\n",
    "        if i == key:\n",
    "            max_int_rate.append(max_values.int_rate[key])\n",
    "            \n",
    "#Checking the minimum interest rate in each state\n",
    "for i in state_name: \n",
    "    for key in min_values['int_rate'].keys():\n",
    "        if i == key:\n",
    "            min_int_rate.append(min_values.int_rate[key])              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_sanctioned_loans = loan_data.groupby('addr_state').size()\n",
    "state_name = []\n",
    "state_value = []\n",
    "full_state_name = []\n",
    "\n",
    "for x,y in total_sanctioned_loans.iteritems():\n",
    "    state_name.append(x)\n",
    "    state_value.append(y)\n",
    "    \n",
    "for i in state_name: \n",
    "    for key in short_state_names.keys():\n",
    "        if i == key:\n",
    "            full_state_name.append(short_state_names[key])         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text1 = pd.DataFrame()\n",
    "\n",
    "#map(float, full_state_name)\n",
    "text1['full_state_name'] = full_state_name[:]\n",
    "text1['mean_loan_amnt'] = mean_value_loan_amnt[:]\n",
    "text1['max_interest_rate'] = max_int_rate[:]\n",
    "text1['min_interest_rate'] = min_int_rate[:]\n",
    "\n",
    "for col in text1.columns:\n",
    "    text1[col] = text1[col].astype(str)\n",
    "    \n",
    "text1['text'] = 'State: ' + text1['full_state_name'] + '<br>' + 'Average \\'loan_amnt\\': ' + text1['mean_loan_amnt'] +\\\n",
    "                '<br>' + 'Max Interest rate: '+ text1['max_interest_rate'] + '<br>' + 'Min Interest rate: ' + text1['min_interest_rate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scl = [0,\"rgb(150,0,90)\"],[0.125,\"rgb(0, 0, 200)\"],[0.25,\"rgb(0, 25, 255)\"],\\\n",
    "[0.375,\"rgb(0, 152, 255)\"],[0.5,\"rgb(44, 255, 150)\"],[0.625,\"rgb(151, 255, 0)\"],\\\n",
    "[0.75,\"rgb(255, 234, 0)\"],[0.875,\"rgb(255, 111, 0)\"],[1,\"rgb(255, 111, 0)\"]\n",
    "\n",
    "#addr_state \n",
    "data = [ dict(\n",
    "        type='choropleth',\n",
    "        colorscale = scl,\n",
    "        autocolorscale = False,\n",
    "        locations = state_name,\n",
    "        z = state_value,\n",
    "        locationmode = 'USA-states',\n",
    "        #text = full_state_name,\n",
    "        text = text1['text'],\n",
    "        marker = dict(\n",
    "            line = dict (\n",
    "                color = 'rgb(255,255,255)',\n",
    "                width = 2\n",
    "            ) ),\n",
    "        colorbar = dict(\n",
    "            title = \"Number of loans sanctioned\")\n",
    "        ) ]\n",
    "\n",
    "layout = dict(\n",
    "        title = 'Number of loans sanctioned per state <br>(Hover for breakdown)',\n",
    "        geo = dict(\n",
    "            scope='usa',\n",
    "            projection=dict( type='albers usa' ),\n",
    "            showlakes = True,\n",
    "            lakecolor = 'rgb(255, 255, 255)'),\n",
    "             )\n",
    "\n",
    "fig = dict( data=data, layout=layout )\n",
    "#py.iplot(fig, filename='map' )          \n",
    "#url = py.plot(fig, validate=False, filename='Loans')\n",
    "plotly.offline.plot(fig, filename='USA_plot')\n",
    "#py.iplot(fig, filename='Median teacher income by State')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rough Analysis\n",
    "Trying to identify data columns that can be deleted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 'id' AND 'member_id' ARE ALL UNIQUE VALUES\n",
    "check_col = [x for x in list(loan_data.columns) if x not in ['id','member_id']]\n",
    "print(check_col)\n",
    "\n",
    "for i in check_col:\n",
    "    print(loan_data.groupby(i).size(), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis results\n",
    "1. Less unique values: term, grade, sub_grade, home_ownership, pymnt_plan, initial_list_status, policy_code (1 group), application_type(2 groups), verification_status_joint(3 groups).\n",
    "2. 'desc' (description) only important for NLP.\n",
    "3. Many features with very less data values, hence need to check for NaN values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Visualization - Null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nan_data = []\n",
    "for i in loan_data.columns:\n",
    "    n_nullvalue = loan_data[str(i)].isnull().sum()\n",
    "    nan_data.append(n_nullvalue)\n",
    "    #print(i,n_nullvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = [go.Bar(x = list(loan_data.columns), y = nan_data, marker=dict(color='#59606D'))]\n",
    "layout = go.Layout(\n",
    "    title = 'Number of Missing Values per Feature',\n",
    "    autosize=False,\n",
    "    width=1000,\n",
    "    height=500,\n",
    "    margin=go.Margin(\n",
    "        l=50,\n",
    "        r=50,\n",
    "        b=250,\n",
    "        t=50,\n",
    "        pad=4\n",
    "    )\n",
    ")\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "plotly.offline.plot(fig, filename='freq_missing_values')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can remove columns which provides very less data insight (here, I chose data columns that have less than 30% of the total data) or has only NULL values.\n",
    "#### Remark:\n",
    "This is not always a good choice because this column could cater to only a specific type of feature, in our case a identifying feature of 'default clients'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "delete_feat = []\n",
    "for i in check_col:\n",
    "    if (sum(loan_data.groupby(i).size()) < (0.3 * len(loan_data))):\n",
    "        #print(i)\n",
    "        delete_feat.append(i)\n",
    "print(len(delete_feat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Visualization - 'loan_status'\n",
    "Visualising the status of the loan takers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loan_status_summary = loan_data.groupby('loan_status').size()\n",
    "print(loan_status_summary)\n",
    "x_loanstatus = []\n",
    "y_loanstatus = []\n",
    "for key,value in loan_status_summary.iteritems():\n",
    "    x_loanstatus.append(key)\n",
    "    y_loanstatus.append(value)\n",
    "    \n",
    "trace = go.Pie(labels = x_loanstatus, values = y_loanstatus)\n",
    "layout = go.Layout(title='Loan Status Values')\n",
    "fig = go.Figure(data=[trace], layout=layout)\n",
    "plotly.offline.plot(fig, filename='loan_status_graph')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Displaying the 'loan_status' in terms of final prediction values: Defaulter or Non-defaulter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "defaulter = 0\n",
    "non_defaulter = 0\n",
    "for i in loan_data.loan_status:\n",
    "    if(i in ['Default','Charged Off','Does not meet the credit policy. Status:Charged Off','Late (31-120 days)']):\n",
    "        defaulter+=1\n",
    "    else:\n",
    "        non_defaulter+=1\n",
    "print(defaulter)\n",
    "print(non_defaulter)\n",
    "\n",
    "trace = go.Pie(labels = ['Defaulters', 'Non-Defaulters'], values = [defaulter,non_defaulter])\n",
    "layout = go.Layout(title='Defaulters Vs Non-defaulters')\n",
    "fig = go.Figure(data=[trace], layout=layout)\n",
    "plotly.offline.plot(fig, filename='defaulters_nondefaulters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Visualization - 'emp_title'\n",
    "Visualising the professions of the loan takers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emp_title_summary = loan_data.groupby('emp_title').size()\n",
    "\n",
    "#'Teacher' AND 'teacher' ARE LISTED AS TWO DIFFERENT PROFESSIONS, UNITE THEM.\n",
    "emp_title_summary['Teacher'] = emp_title_summary['Teacher'] + emp_title_summary['teacher']\n",
    "emp_title_summary['teacher'] = 0\n",
    "emp_title_summary = emp_title_summary.sort_values(ascending=False)\n",
    "print(emp_title_summary)\n",
    "\n",
    "x_professions = []\n",
    "y_n_professions = []\n",
    "for key,value in emp_title_summary.iteritems():\n",
    "    x_professions.append(key)\n",
    "    y_n_professions.append(value) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of professions listed. Hence, we only look at a few common ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trace = go.Pie(labels = x_professions[:30], values = y_n_professions[:30])\n",
    "layout = go.Layout(title=('Professions <br> (Hover for breakdown)'))\n",
    "fig = go.Figure(data=[trace], layout=layout)\n",
    "plotly.offline.plot(fig, filename='professions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Visualization - 'title' \n",
    "Visualising the major reason why people took the loan, in terms of a word cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words = list()\n",
    "words.append(loan_data.title.tolist())\n",
    "sentence = ''.join(' %s' % word for word in words[0])\n",
    "STOPWORDS.add('consolidation')\n",
    "STOPWORDS.add('Consolidation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#wordcloud = WordCloud(min_font_size=14, background_color='white',).generate(sentence)\n",
    "wordcloud = WordCloud(font_path=None, width=2000, height=1000, margin=4, ranks_only=None,\n",
    "                   prefer_horizontal=0.7, scale=1, max_words=2000, min_font_size=15,\n",
    "                   stopwords=STOPWORDS, random_state=None, background_color='white', max_font_size=None,\n",
    "                   font_step=1, mode='RGB', relative_scaling=0).generate(sentence[:1000000])\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reason_summary = loan_data.groupby('title').size()\n",
    "\n",
    "x_reason = []\n",
    "y_n_reason = []\n",
    "for key,value in reason_summary.iteritems():\n",
    "    x_reason.append(key)\n",
    "    y_n_reason.append(value) \n",
    "\n",
    "trace = go.Pie(labels = x_reason[:30], values = y_n_reason[:30])\n",
    "layout = go.Layout(title=('Reason for taking Loan <br> (Hover for breakdown)'))\n",
    "fig = go.Figure(data=[trace], layout=layout)\n",
    "plotly.offline.plot(fig, filename='reason_loan')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2.  Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a new dataframe to work with: 'data2'.\n",
    "Delete some columns:\n",
    "1. Deleting the columns that provide less data insight.\n",
    "2. Deleting columns with high frequency of Null values (not now, but maybe after feature importance calculation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data2 = loan_data[:]\n",
    "data2.drop(delete_feat, inplace=True, axis=1)\n",
    "\n",
    "#CAN FURTHER REMOVE SOME COLUMNS ACCORDING TO ROUGH ANALYSIS\n",
    "data2.drop('acc_now_delinq', inplace=True, axis=1) #Mostly always 0\n",
    "data2.drop('policy_code', inplace=True, axis=1) #Always 1\n",
    "\n",
    "#DELETING 'desc', 'url', 'title'. BETTER FOR NLP OR SEMANTIC WEB MINING\n",
    "data2.drop('url', inplace=True, axis=1)\n",
    "data2.drop('desc', inplace=True, axis=1)\n",
    "data2.drop('title', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#CONVERT 'term' FROM STRING TO INT, REMOVING THE 'months' WORD\n",
    "data2.term = loan_data.term.str.split(\" \").str[1]\n",
    "\n",
    "#REMOVE '%' FROM 'int_rate' TO CONVERT IT INTO NUMBERIC DATA\n",
    "#print(data2.int_rate.dtypes)\n",
    "data2.int_rate = loan_data['int_rate'].str.split(\"%\").str[0]\n",
    "data2.int_rate = data2.int_rate.astype(float)/100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(list(data2.select_dtypes(include=['object']).columns))\n",
    "\n",
    "for i in list(data2.select_dtypes(include=['object']).columns):\n",
    "    data2.i = pd.to_numeric(data2[str(i)],errors='ignore')\n",
    "print(list(data2.select_dtypes(include=['object']).columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#WORKING WITH 'emp_length' VALUE\n",
    "data2.emp_length = data2['emp_length'].str.extract('(\\d+)').astype(float)\n",
    "#print(data2.emp_length)\n",
    "data2.emp_length.isnull().values.any()\n",
    "\n",
    "#NULL VALUES PRESENT IN THE COLUMN, HENCE FILLING IT WITH MEDIAN VALUE\n",
    "data2.emp_length = data2.emp_length.fillna(data2.emp_length.median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deleting all columns that have 'NaN' value.\n",
    "#### Remark: \n",
    "Not a good choice always, but we shall get back to this when modeling if the results aren't good (indicating maybe we dropped some important features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#CHECKING WHAT COLUMNS HAVE 'NaN' VALUES\n",
    "nan_column = []\n",
    "for i in data2.columns:\n",
    "    if (data2[str(i)].isnull().any().any()):\n",
    "        #print(i)\n",
    "        nan_column.append(i)\n",
    "print(nan_column)\n",
    "\n",
    "data2.drop(nan_column, inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying Label encoding for other features (columns) to convert into numerical values. <br>\n",
    "Doing so, allows us to use these features to fed for feature importance calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(list(data2.select_dtypes(include=['object']).columns))\n",
    "print(data2.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lb_make = LabelEncoder()\n",
    "\n",
    "#grade\n",
    "data2['grade'] = lb_make.fit_transform(data2['grade'])\n",
    "#print(data2.groupby(data2.grade).size())\n",
    "\n",
    "data2['sub_grade'] = lb_make.fit_transform(data2['sub_grade'])\n",
    "\n",
    "#Not too profitable since there are still many classes/groups\n",
    "#data2['emp_title'] = data2['emp_title'].factorize()[0]\n",
    "#data2['emp_title'] = lb_make.fit_transform(data2['emp_title'])\n",
    "\n",
    "#home_ownership: [(ANY,0),(MORTGAGE,1),(NONE,2),(OTHER,3),(OWN,4),(RENT,5)]\n",
    "data2.home_ownership = lb_make.fit_transform(data2.home_ownership)\n",
    "\n",
    "#issue_d\n",
    "data2.issue_d = lb_make.fit_transform(data2.issue_d)\n",
    "\n",
    "#verification_status : [(Not Verified,0),(Source Verified,1),(Verified,2)]\n",
    "data2.verification_status = lb_make.fit_transform(data2.verification_status)\n",
    "\n",
    "#loan_status : [(Not Charged Off,0),(Current,1),(Default,2),(Does not meet the credit policy. Status:Charged Off,3),(Does not meet the credit policy. Status:Fully Paid ,4),(Fully Paid,5),(In Grace Period,6),(Issued,7),(Late (16-30 days),8),(Late (31-120 days),9)]\n",
    "data2.loan_status = lb_make.fit_transform(data2.loan_status)\n",
    "\n",
    "#pymnt_plan : [(n,0),(y,1)]\n",
    "data2.pymnt_plan = lb_make.fit_transform(data2.pymnt_plan)\n",
    "\n",
    "#purpose : [(car,0),(credit_card,1),(debt_consolidation,2),(educational,3),(home_improvement,4),(house,5),(major_purchase,6),(medical,7),(moving,8),(other,9),(renewable_energy,10),(small_business,11),(vacation,12),(wedding,13)]\n",
    "data2.purpose = lb_make.fit_transform(data2.purpose)\n",
    "\n",
    "#zip_code (935)\n",
    "data2.zip_code = lb_make.fit_transform(data2.zip_code)\n",
    "\n",
    "#addr_state\n",
    "data2.addr_state = lb_make.fit_transform(data2.addr_state)\n",
    "\n",
    "#earliest_cr_line\n",
    "#Nan values are being factorized and assigned -1 value\n",
    "#data2['earliest_cr_line'] = data2['earliest_cr_line'].factorize()[0]\n",
    "#data2.earliest_cr_line = lb_make.fit_transform(data2.earliest_cr_line)\n",
    "\n",
    "#initial_list_status : [(f,0),(w,1)]\n",
    "data2.initial_list_status = lb_make.fit_transform(data2.initial_list_status)\n",
    "\n",
    "#last_pymnt_d \n",
    "#data2['last_pymnt_d'] = data2['last_pymnt_d'].factorize()[0]\n",
    "#data2.last_pymnt_d = lb_make.fit_transform(data2.last_pymnt_d)\n",
    "\n",
    "#next_pymnt_d\n",
    "#data2['next_pymnt_d'] = data2['next_pymnt_d'].factorize()[0]\n",
    "#data2.next_pymnt_d = lb_make.fit_transform(data2.next_pymnt_d)\n",
    "\n",
    "#last_credit_pull_d\n",
    "#data2['last_credit_pull_d'] = data2['last_credit_pull_d'].factorize()[0]\n",
    "#data2.last_credit_pull_d = lb_make.fit_transform(data2.last_credit_pull_d)\n",
    "\n",
    "#application_type : [(INDIVIDUAL,0),(JOINT,1)]\n",
    "data2.application_type = lb_make.fit_transform(data2.application_type)\n",
    "\n",
    "#verification_status_joint : [(Not Verified,0),(Source Verified,1),(Verified,2)]\n",
    "#data2['verification_status_joint'] = data2['verification_status_joint'].factorize()[0]\n",
    "#data2.verification_status_joint = lb_make.fit_transform(data2.verification_status_joint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X -> Features to use <br>\n",
    "Y -> Prediction value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y = data2['loan_status']\n",
    "X_unscaled = data2[:]\n",
    "X_unscaled.drop('loan_status', inplace = True, axis = 1)\n",
    "X = preprocessing.scale(X_unscaled)\n",
    "#print(X_unscaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Split (Training and Testing)\n",
    "70% -> Training <br>\n",
    "30% -> Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Splitting into test and training dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, shuffle = True)\n",
    "#print(X_train.shape)\n",
    "#print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = ExtraTreesClassifier(random_state=10)\n",
    "clf = clf.fit(X_train, y_train)\n",
    "importances = clf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_to_drop = (importances <= 0.01)\n",
    "features_indexes, = np.where(features_to_drop == True)\n",
    "print(features_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = [go.Bar(x = data2.columns, y = clf.feature_importances_)]\n",
    "layout = go.Layout(\n",
    "    title='Feature Importances',\n",
    "    autosize=False,\n",
    "    width=1000,\n",
    "    height=500,\n",
    "    margin=go.Margin(\n",
    "        l=50,\n",
    "        r=50,\n",
    "        b=250,\n",
    "        t=50,\n",
    "        pad=4\n",
    "    )\n",
    ")\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "plotly.offline.plot(fig, filename='feature_importances')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = SelectFromModel(clf, threshold = 0.01, prefit=True)\n",
    "dataset = model.transform(X)\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3.  Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Conversion into Binary Classes\n",
    "Defaulter (0) -> (Charged Off,0), (Default,2), (Does not meet the credit policy. Status:Charged Off,3), (Late (31-120 days),9)\n",
    "<br> Non_defaulter (1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_binary = []\n",
    "for i in Y:\n",
    "    if(i in [2,3,9,0]):\n",
    "        Y_binary.append(0)\n",
    "    else:\n",
    "        Y_binary.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_binary = np.array(Y_binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1. Random Forest\n",
    "Here Y is taken into different classes and not binary classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(n_estimators = 15)\n",
    "k_fold = KFold(n_splits = 4)\n",
    "\n",
    "for train_ind, test_ind in k_fold.split(dataset):\n",
    "    X_train, X_test = dataset[train_ind], dataset[test_ind]\n",
    "    y_train, y_test = Y_binary[train_ind], Y_binary[test_ind]\n",
    "    rfc = rfc.fit(X_train, y_train)\n",
    "    y_pred = rfc.predict(X_test)\n",
    "    y_score = rfc.predict_proba(X_test)[:,1]\n",
    "        \n",
    "    print(\"accuracy : \",rfc.score(X_test, y_test)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols = ['model','matthews_corrcoef', 'roc_auc_score', 'precision_score', 'recall_score','f1_score']\n",
    "\n",
    "models_report = pd.DataFrame(columns = cols)\n",
    "conf_matrix = dict()\n",
    "\n",
    "#print('computing {} - {} '.format(clf_name, model_type))\n",
    "tmp = pd.Series({'model': 'Random Forest Classifier',\n",
    "                 'roc_auc_score' : metrics.roc_auc_score(y_test, y_score),\n",
    "                 'matthews_corrcoef': metrics.matthews_corrcoef(y_test, y_pred),\n",
    "                 'precision_score': metrics.precision_score(y_test, y_pred),\n",
    "                 'recall_score': metrics.recall_score(y_test, y_pred),\n",
    "                 'f1_score': metrics.f1_score(y_test, y_pred)})\n",
    "\n",
    "models_report = models_report.append(tmp, ignore_index = True)\n",
    "conf_matrix[rfc] = pd.crosstab(y_test, y_pred, rownames=['True'], colnames= ['Predicted'], margins=False)\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, y_score, drop_intermediate = False, pos_label = 1)\n",
    "\n",
    "plt.figure(1, figsize=(6,6))\n",
    "plt.xlabel('false positive rate')\n",
    "plt.ylabel('true positive rate')\n",
    "#plt.title('ROC curve - {}'.format(model_type))\n",
    "plt.plot(fpr, tpr, label = 'RFC')\n",
    "plt.legend(loc=2, prop={'size':11})\n",
    "plt.plot([0,1],[0,1], color = 'black')\n",
    "    \n",
    "print(models_report, conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Splitting into test and training dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset, Y_binary, test_size=0.3, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2. Gradient Boosting regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {'n_estimators': 1000, 'max_depth': 4, 'min_samples_split': 3, 'learning_rate': 0.01, 'loss': 'ls'}\n",
    "clf = ensemble.GradientBoostingRegressor(**params)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "mse = mean_squared_error(y_test, clf.predict(X_test))\n",
    "print(\"MSE: %.4f\" % mse)\n",
    "print(clf.score(X_test, y_test)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3. Neural Networks (Multi layer perceptron)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(solver='sgd', alpha=1e-5, hidden_layer_sizes=(15,10,5),random_state=1, learning_rate = 'adaptive')\n",
    "#mlp.fit(X_train, y_train)\n",
    "#print(mlp.score(X_test,y_test))\n",
    "\n",
    "k_fold = KFold(n_splits = 4)\n",
    "\n",
    "for train_ind, test_ind in k_fold.split(dataset):\n",
    "    X_train, X_test = dataset[train_ind], dataset[test_ind]\n",
    "    y_train, y_test = Y_binary[train_ind], Y_binary[test_ind]\n",
    "    \n",
    "    mlp = mlp.fit(X_train, y_train)\n",
    "    y_pred = mlp.predict(X_test)\n",
    "    y_score = mlp.predict_proba(X_test)[:,1]\n",
    "    print(\"accuracy : \", mlp.score(X_test, y_test)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols = ['model','matthews_corrcoef', 'roc_auc_score', 'precision_score', 'recall_score','f1_score']\n",
    "\n",
    "models_report = pd.DataFrame(columns = cols)\n",
    "conf_matrix = dict()\n",
    "\n",
    "tmp = pd.Series({'model': 'Multi layer Perceptron',\n",
    "                 'roc_auc_score' : metrics.roc_auc_score(y_test, y_score),\n",
    "                 'matthews_corrcoef': metrics.matthews_corrcoef(y_test, y_pred),\n",
    "                 'precision_score': metrics.precision_score(y_test, y_pred),\n",
    "                 'recall_score': metrics.recall_score(y_test, y_pred),\n",
    "                 'f1_score': metrics.f1_score(y_test, y_pred)})\n",
    "\n",
    "models_report = models_report.append(tmp, ignore_index = True)\n",
    "conf_matrix[mlp] = pd.crosstab(y_test, y_pred, rownames=['True'], colnames= ['Predicted'], margins=False)\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, y_score, drop_intermediate = False, pos_label = 1)\n",
    "\n",
    "plt.figure(1, figsize=(6,6))\n",
    "plt.xlabel('false positive rate')\n",
    "plt.ylabel('true positive rate')\n",
    "#plt.title('ROC curve - {}'.format(model_type))\n",
    "plt.plot(fpr, tpr, label = 'MLP')\n",
    "plt.legend(loc=2, prop={'size':11})\n",
    "plt.plot([0,1],[0,1], color = 'black')\n",
    "    \n",
    "print(models_report, conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 4. Neural Network (Tensorflow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_targets = len(np.unique(Y_binary))\n",
    "\n",
    "#Our Learning  Parameters\n",
    "learning_rate = 0.001\n",
    "num_epochs = 8\n",
    "batch_size = 100\n",
    "display_step = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "  initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "  initial = tf.constant(0.1, shape=shape)\n",
    "  return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32,[None,15])\n",
    "y = tf.placeholder(tf.float32,[None,2])\n",
    "w1 = weight_variable([15,200])\n",
    "b1 = bias_variable([200])\n",
    "w2 = weight_variable([200,500])\n",
    "b2 = weight_variable([500])\n",
    "\n",
    "sm_w = weight_variable([500,n_targets])\n",
    "sm_b = bias_variable([n_targets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "affine = tf.nn.relu(tf.matmul(x,w1)+b1)\n",
    "affine2 = tf.nn.relu(tf.matmul(affine,w2)+b2)\n",
    "drop_prob = tf.placeholder(\"float\")\n",
    "drop_out = tf.nn.dropout(affine2, drop_prob)\n",
    "sm_affine = tf.matmul(drop_out,sm_w)+sm_b\n",
    "sm = tf.nn.softmax(sm_affine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Loss function\n",
    "cost = -tf.reduce_sum(y*tf.log(sm))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(sm, 1), tf.argmax(y, 1))\n",
    "#Calculate accuracy\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init=tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Splitting into test and training dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset, Y_binary, test_size=0.3, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "with tf.Session() as session:\n",
    "    session.run(init) # initalize\n",
    "    for i in range(num_epochs):\n",
    "        batch_size=100\n",
    "        nb = X_train.shape[0]/ batch_size;\n",
    "        print(\"epoch : \", i+1)\n",
    "        pre_batch =0\n",
    "        for j in range(int(nb)):\n",
    "            if batch_size <= X_train.shape[0]:\n",
    "                xs = X_train[pre_batch:batch_size]\n",
    "                ys = y_train[pre_batch:batch_size]\n",
    "                ys = ys.reshape((100,1))\n",
    "                pre_batch = batch_size\n",
    "                batch_size += 100\n",
    "                #print batch_size\n",
    "                session.run(optimizer,feed_dict = {x:xs,y:ys,drop_prob:0.5})\n",
    "                cost_per_batch = session.run(cost,feed_dict = {x:xs,y:ys,drop_prob:0.5})\n",
    "            \n",
    "    print(\"Optimization Finished\")\n",
    "    saver.save(session,'model.ckpt')\n",
    "    y_test = y_test.reshape((y_test.shape[0],1))\n",
    "    #print (correct_prediction.eval({x: X_test, y: y_test,drop_prob :1.0}))\n",
    "    print (\"Accuracy:\", accuracy.eval({x: X_test, y: y_test,drop_prob :1.0})*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
